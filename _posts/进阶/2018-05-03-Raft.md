---
layout: post
title:   Raft
date:   2018-05-03 19:15:10
categories:  进阶
tags: 分布式
keywords: Netty
description: 
---

## Raft

首先推荐一个Raft的动态演示页面，很清晰的演示了Raft的执行流程：[Raft动态演示](http://thesecretlivesofdata.com)

Raft作为一个分布式一致性协议，它实际上它们使用了和Multi Paxos完全一样的原理来保证协议的安全性

## 正常状态

一个Raft集群通常包括多个节点以保证高可用，例如对于包含五个节点的集群，可以容忍两个节点出现故障。下图是一个正常工作状态的三个节点的Raft集群：

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft.png)

说明：
* 一个Raft集群只包含一个Leader节点，其余均为Follower节点；
* 每个节点有3种状态，Leader、Follower、Candidate；
* 客户端只允许和Leader节点进行交互；
* Follower节点只允许从Leader节点接收写日志的请求（AppendEntries RPC）；
* Leader节点将客户端的请求发送给所有的Follower节点，只有至少一半的节点回复OK时，才可以将日志commit，并返回给用户成功；
* Leader节点定期向所有的Follower节点发送心跳信息（不包含实际操作的 AppendEntries RPC），来告诉它们自己依然健康；

这里我们可以看出，Raft协议由Leader来确定日志的写入顺序（实际上也是操作执行的顺序），而Follower节点只要接收Leader节点的写日志请求并将日志追加写入即可。

## 选主（Leader Election）

最开始还没有Leader产生的时候，每个节点都有个election timeout,不同节点的election timeout时间不同（一般随机在150ms and 300ms之间），一旦某个节点先跑完election timeout，这个节点就会变成Candidate状态，开始竞选过程。

下面Node C最先跑完election timeout，于是变成Candidate，并给自己投了一票，任期为1；

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft1.PNG)

Candidate节点给自己投1票，同时向其他所有节点发送拉票请求（RequestVote RPC），A B节点投票之后会重置自己的election timeout，收到多数派投票的Candidate会变成Leader：

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft2.PNG)

Node C变成Leader后会每指定间隔就给A B节点发送心跳包证明自己存活，A B节点也会回复一个Append Entries RPC，如果在Follower一次heartbeat timeout内没收到Leader的心跳包，就认为Leader挂了，自己变成Candidate（raft有两个timeout，heartbeat timeout和 election timeout，不同节点的heartbeat timeout也不同）

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft3.PNG)

当Leader节点由于异常原因（宕机、网络故障等）无法继续提供服务时，可以认为它结束了本轮任期（Term=n），需要开始新一轮的选举（Election），而新的Leader当然要从Follower中产生，开始新一轮的任期（Term=n+1）。

然后重复上面的流程，B先跑完heartbeat timeout没有收到心跳包，就变成Candidate，然后选举为Leader

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft4.PNG)

需要大多数投票保证了每个任期内只有一个Leader产生，如果同时有两个Candidate同时产生就会发生split vote的情况；

### Split Vote

如下图，同时两个节点近乎同时结束election timeout,都同时给其余3个节点发出拉票请求（RequestVote RPC）

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft5.PNG)

假设A节点把选票投给B，D节点把选票投给C，这样B C都分别各自有2个选票，这样无法遵守多数派原则选出Leader。  这个时候就会开始新的一轮election，开启新的一轮Term。

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft6.PNG)

这次A节点先跑完election timeout，最先成为Candidate，然后成为Leader。

## 数据写入（ Log Replication）

比如Client向Leader写入一个SET 5的log,注意，这个时候Leader并不会马上将自己的值改写为5。

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft7.PNG)

这次写入会在下次Leader发送心跳包的时候，通过心跳包发送给Follower询问这次请求是否能接受,只有在大多数Follower接受这次写入并返回一次RPC告知Leader的时候，Leader才将自己的改成5，然后Leader返回给Client一个response告诉Client这次写入是OK的，并在下次传输心跳包的时候将5的值写入到每个Follower节点。

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft8.PNG)

### 一个问题
```
假设5个节点Raft集群：[s1, s2, s3, s4, s5] ，设s1为Leader，其余为Follower

第1个操作：SET x = 0
    所有节点均更新成功：s1(x=0), s2(x=0), s3(x=0), s4(x=0), s5(x=0)
第2个操作：SET x = x + 1
    s5由于网络原因更新失败：s1(x=1), s2(x=1), s3(x=1), s4(x=1), s5(x=0)
第3个操作：SET x = x + 2
    s5恢复正常：s1(x=3), s2(x=3), s3(x=3), s4(x=3), s5(x=2)

集群内节点的状态已经不一致了！不一致的主要原因是，除了s5以外的其他节点读取的日志序列是[x=0, x=x+1, x=x+2]，而s5读取的日志序列为[x=0, x=x+2]
```
Raft是使用log来同步状态机状态的，我们先看看看log的结构：

![enter image description here](http://p7lixluhf.bkt.clouddn.com/raft9.png)

说明：
* 每一条日志都有日志序号（log index）
* 每一条日志记录除了保存 state machine 要执行的操作（如 x ← 0）外，还需要保存该操作对应的 Term
* 由Leader决定什么时候对记录进行commit：当一条记录已经在至少半数的Follower上持久化后，Leader可以将该记录commit，并提供给state machine执行对应的操作
* Leader给Follower的复制日志请求中（AppendEntries RPC），不仅包含具体的操作，还包含本次插入的前一个位置的index和Term；这样对于刚才的问题，s5节点就不会直接追加日志了，而会检查发现自己有日志缺失，进而由Leader将缺失的日志全部同步过来（实际上Leader里记录了发送给每一个Follower的日志序号）；

总结就是：Leader负责一致性检查，同时让所有的Follower都和自己保持一致。Raft保证被选为新leader的节点拥有所有已提交的log entry

Raft对选举有限制条件
* 限制条件1：Candidate在拉票时需要携带自己本地已经持久化的最新的日志信息，等待投票的节点如果发现自己本地的日志信息比竞选的Candidate更新，则拒绝给他投票。
* 限制条件2：只允许Leader提交（commit）当前Term的log。

## Log Compaction

在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响availability。Raft采用对整个系统进行snapshot来处理，snapshot之前的日志都可以丢弃。Snapshot技术在Chubby和ZooKeeper系统中都有采用。

Raft使用的方案是：每个副本独立的对自己的系统状态进行Snapshot，并且只能对已经提交的日志记录（已经应用到状态机）进行snapshot。

Snapshot中包含以下内容：

* 日志元数据，最后一条commited log entry的 (log index, last_included_term)。这两个值在Snapshot之后的第一条log entry的AppendEntries RPC的consistency check的时候会被用上，之前讲过。一旦这个server做完了snapshot，就可以把这条记录的最后一条log index及其之前的所有的log entry都删掉。
* 系统状态机：存储系统当前状态（这是怎么生成的呢？）


snapshot的缺点就是不是增量的，即使内存中某个值没有变，下次做snapshot的时候同样会被dump到磁盘。当leader需要发给某个follower的log entry被丢弃了(因为leader做了snapshot)，leader会将snapshot发给落后太多的follower。或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用新的RPC，InstalledSnapshot。

做snapshot有一些需要注意的性能点
* 1. 不要做太频繁，否则消耗磁盘带宽。
*  2. 不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。系统推荐当日志达到某个固定的大小做一次snapshot。
*  3. 做一次snapshot可能耗时过长，会影响正常log entry的replicate。这个可以通过使用copy-on-write的技术来避免snapshot过程影响正常log entry的replicate。